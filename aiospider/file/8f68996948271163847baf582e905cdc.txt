The extent to which “smart city” technology is altering our sense of freedom in public spaces deserves more attention if we want a democratic future. Democracy–the rule of the people–constitutes our collective self-determination and protects us against domination and abuse. Democracy requires safe spaces, or commons, for people to organically and spontaneously convene regardless of their background or position to campaign for their causes, discuss politics, and protest. In these commons, where anyone can take a stand and be noticed is where a notion of collective good can be developed and communicated. Public spaces, like our streets, parks, and squares, have historically played a significant role in the development of democracy. We should fight to preserve the freedoms intrinsic to our public spaces because they make democracy possible.Last summer, approximately 15 to 26 million people participated in Black Lives Matter protests after the murder of George Floyd making it the largest mass movement in U.S. history. In June, the San Diego Police Department obtained footage of Black Lives Matter protesters from “smart streetlight” cameras, sparking shock and outrage from San Diego community members. These “smart streetlights” were promoted as part of citywide efforts to become a “smart city” to help with traffic control and air quality monitoring. Despite discoverable documentation about the streetlight’s capabilities and data policies on their website, including a data-sharing agreement about how they would share data with the police, the community had no expectation that the streetlights would be surveilling protestors. After media coverage and ongoing advocacy from the Transparent and Responsible Use of Surveillance Technology San Diego (TRUSTSD) coalition, the City Council, set aside the funding for the streetlights until a surveillance technology ordinance was considered and the Mayor ordered the 3,000+ streetlight cameras off. Due to the way power was supplied to the cameras, they remained on, but the city reported it no longer had access to the data it collected. In November, the City Council voted unanimously in favor of a surveillance ordinance and to establish a Privacy Advisory Board. In May, it was revealed that the San Diego Police Department had previously (in 2017) held back materials to Congress’ House Committee on Oversight and Reform about their use facial recognition technology. This story, with its mission creep and mishaps, is representative of a broader set of “smart city” cautionary trends that took place in the last year. These cautionary trends call us to question if our public spaces become places where one fears punishment, how will that affect collective action and political movements?This report is an urgent warning of where we are headed if we maintain our current trajectory of augmenting our public space with trackers of all kinds. In this report, I outline how current “smart city” technologies can watch you. I argue that all “smart city” technology trends toward corporate and state surveillance and that if we don’t stop and blunt these trends now that totalitarianism, panopticonism, discrimination, privatization, and solutionism will challenge our democratic possibilities. This report examines these harms through cautionary trends supported by examples from this last year and provides 10 calls to action for advocates, legislatures, and technology companies to prevent these harms. If we act now, we can ensure the technology in our public spaces protect and promote democracy and that we do not continue down this path of an elite few tracking the many. “Smart city” technology surfaced as a concept more than 20 years ago and serves as an umbrella term for a wide range of technologies collecting and transmitting data in the city environment. For simplicity, this report will focus on “smart city” technology that is capable of collecting data that can identify individuals because that data can be used to target individuals, which in turn can erode the sense of safety and inclusivity requisite for public spaces to serve as commons for democratic functions.Many types of technology may fall under the umbrella term of “smart city” technology. This report will focus primarily on the hardware and software associated with cameras, location trackers, and sensors. These technologies are common components in broader “smart city” technologies and projects, and they have the high-risk ability to collect data that can directly, or in combination with other data, identify individuals. Technologies all have inherent affordances, that is qualities or properties that define their possible uses or clarify how they can or should be used. These technology components all have the inherent capabilities to track individuals. Simply put, these are devices that may be watching you.Security cameras in public spaces first became a common practice in the 1970s and 1980s. The proliferation of these cameras, their capacity to identify individuals, and their uses by law enforcement are steadily increasing. Public Closed-Circuit Television (CCTV) cameras come in various shapes and sizes and may be attached to various government-owned fixtures such as buildings, telephone poles, and traffic lights. In addition, cameras may be inconspicuously attached to unmanned aerial vehicles (UAVs) like drones or spy planes or built into other devices like kiosks, mirrors, parking meters, poles, scooters, street lights, and traffic lights. Lastly, law enforcement may be wearing body-worn cameras. In addition to government-owned cameras, privately-owned cameras, including cameras built into cell phones, may be filming you. These cameras may have real-time biometric recognition software built into the device itself, such as facial recognition, fingerprint recognition, iris recognition, gait recognition, and tattoo recognition. These sets of software are often also referred to as “technologies.”Cell phones and various transportation-related technologies collect location data. If governments or corporations collect this data at an individual level, it can be used to identify individuals and track their every move. Cell phones can disclose your location in many ways, including mobile signal tracking from towers, mobile signal tracking from cell-site simulators, Wi-Fi, Bluetooth, and location information from applications and web browsing. Bikeshare, rideshare (including taxis), and scooters can collect individual trip data via GPS or their associated cell phone application and privately owned vehicles can provide telematics data or “vehicle forensics kits” to third parties. Public transit can collect individual trip data and link it to your transit card or personal financial information. Mobility data can be collected by third-party devices such as Automated License Plate Readers (ALPRs), Intelligent Transportation Systems, or cell phone location data brokers. ALPRs are commonly mounted on tow trucks, law enforcement cars, and surveillance cameras. In addition to cameras and sensors, Intelligent Transportation Systems can include induction loops, infrared, radar, sound or video imaging, or Bluetooth that collect mobility data.Sensors convert stimuli such as heat, light, sound, and motion into electrical signals and can be used to identify individuals’ presence or identifying qualities. Some common applications of using these sensors to watch people in cities have been the use of audio sensors (like ShotSpotter) to detect and analyze audio signals, infrared sensors, Light Detection and Ranging (LiDAR) sensors, motion sensors (that use microwave, reflective, ultrasonic, or vibration sensing) to detect moving people and vehicles, and thermal sensors which are commonly used to detect the heat of suspects or victims by law enforcement.Other “smart city” technology items that are not covered in the broad three categories above that may be collecting data that can identify individuals include Information and Communications Technology (ICT) infrastructure (cell phone towers powering up to 5G and public Wi-Fi), Internet of Things (IoT) or GPS-system connectivity to various things such as electronic monitoring, “smart tags” and RFID Chips, online activity (of government websites, financial transactions, social media analysis), police robots (like Boston Dynamics’ digidogs or Knightscope’s rolling pickles), and “smart kiosks” and USB ports.What data cameras, location trackers, and sensors collect, whether or not it is being analyzed in real-time, and how it is managed throughout its life all affect its likelihood of creating risks for individuals. Identifying data exists on a spectrum. For example, data with “direct identifiers” such as your name, biometric, or address can readily identify you. This data type has historically been categorized as personally identifiable information (PII) and is the riskiest data for the technologies above to collect. Further along the spectrum, and the next riskiest are “indirect identifiers” that combine with other data to identify you—for example, your IP address, geolocation, or license plate number. Further still along the spectrum are data that can be ambiguously linked to multiple people—for example, cars detected by a motion detector.This data by itself may not be able to identify you, but in certain contexts it may be able to. As more data is collected and data joining techniques become more sophisticated, so does the ability to re-identify someone within a seemingly ambiguous dataset. This ability is commonly referred to as the “mosaic effect” derived from the mosaic theory of intelligence gathering, in which “disparate pieces of information—although individually of limited utility—become significant when combined with other types of information.” As re-identifying risks increase, data practitioners are challenged with the trade-offs of collecting granular data, which can be of high research utility and protecting against re-identification harms.Throughout this report, I will refer to data that can directly or indirectly or through re-identification techniques identify an individual as “identifying data.” I will argue that in the “smart city” context, identifying data collection can lead to harms that threaten democracy, so the question for communities will be, what utility is worth that trade-off? Further down the identifying data spectrum is data that cannot be linked to a specific person, such as aggregated taxi rides in a year. And lastly, on the spectrum is data that is not linked to individuals, like weather reports.Whether or not identifying data is analyzed in real-time, as opposed to historical snapshots, affects its riskiness because it allows for real-time targeting. Lastly, beyond the collection of identifying data, how these datasets are managed throughout the entire data lifecycle (stored, accessed, analyzed, joined, etc.) can expand the ability to identify, and thus target, individuals.While the above technologies can collect data that can be used to identify you, it does not necessarily mean that they are or have to. For example, cameras can be replaced with motion detectors or configured with “video anonymization software” to collect blurred images. Images can also be altered after collection to better protect against re-identification with image-altering tools that blur or pixelate and strip identifying metadata or use generative adversarial network (GAN) escape detection techniques to create fake derivative images that look similar to the naked eye. (These post-collection techniques may not successfully protect against the risks of re-identification by highly skilled technologists now or in the future as traces of the original image may be detectable.) On the other hand, cameras can be augmented with facial recognition technology which can use algorithms to match a crisp image of your passing face with a known image of you. Relatively new, facial recognition technology got twenty times better at recognizing a person out of a collection of millions of photos between 2014 and 2018. In addition, the technology has become much more affordable. Until recently, it would be a safe assumption to remain anonymous in public spaces unless you ran into someone you knew. If facial recognition technology is scanning public spaces at all times, that is no longer a safe assumption and changes the nature of public spaces and how we will operate in them. At the farthest end of the spectrum, cameras can be augmented with real-time biometric recognition capabilities to quickly identify you and lookup related datasets about you to form a composite of who you are. These big data systems are already being used, such as in Kashgar, where a state-run defense manufacturer, China Electronics Technology Corporation, runs a high-tech surveillance system to monitor and subdue millions of Uyghurs and members of other Muslim ethnic groups. Examples of these developing big data systems in the U.S. include advanced analytics promising “digital twins,” predictive policing, fusion centers or real-time crime centers, and video analytics. IDEO CoLab illustrated this range of data possibilities for cameras in an interactive art exhibit. SeeScreengrabs from Vanishing Points: An IDEO CoLab Prototype, a Collaborative Cities concept from the April 2019 IDEO CoLab design sprints depicting how cameras can be collecting different types of data.Location trackers can range in granularity of data between individual trips and aggregate yearly trips. Personal trip data can easily be used to connect you to specific sensitive trips. For example, when a trip begins at a sensitive location such as a political protest, all the government would need to know is who lives at the house at the end of the trip to identify them and note their involvement in the demonstration. Location trackers can range in how location data is joined with other datasets. For example, cell phone applications sometimes include subcomponent software, like X-Mode, that can track your location across applications to create a complete dataset of your location at all times.Sensors range in how they sense people, sometimes collecting identifying data such as biometrics, re-identifying data such as drug use in water, or aggregate foot traffic data while generating energy. While many sensors are not directly aimed at tracking individuals, it does not mean that information collected from them could not be paired with other data streams like facial recognition and artificial intelligence (AI) against an individual. Steve Bellovin, a computer science professor at Columbia University, offered the  the following hypothetical: “might a pollution sensor detect cigarette smoke or vaping, while a Bluetooth receiver picks up the identities of nearby phones? Insurance companies might be interested.” Still, other sensors may be collecting data about the environment, such as precipitation sensors, that cannot be used to identify anyone.While certain technologies create dangerous affordances and identifying data creates inherent risks to individuals, how this data is legally permitted to be accessed and used is of great significance to societal power structures and effects on democracy. Today, “smart city” technology, often sold under a banner of “collective goods,” is knowingly or unknowingly being repurposed by law enforcement for widespread surveillance and punitive purposes. Specific examples of this mission creep will be detailed further in the report.To minimize the harms of this technology, some jurisdictions have begun to regulate facial recognition technology and surveillance technology. To reduce the harms that can arise from collecting identifying data, some jurisdictions have started to regulate the collection of biometrics, consumer data, and general data protection, like the European Union’s (EU) General Data Protection Regulation (GDPR), and specific other datasets. Finally, to minimize the harms of these uses, some jurisdictions have begun regulating how this data can be used after it is collected, such as banning the use of facial recognition databases like ClearviewAI in Canada or the sharing of personal transit data by transit agencies without a warrant in Massachusetts. Many of these laws regulate all three aspects of technology, data, and use to some extent. Still, I have found it helpful to think about these items distinctly because they each have different ways of creating harm and different regulatory challenges, which will become evident as we explore these harms and interventions further.Below I have briefly outlined cautionary trends of where “smart city” technology may be headed. No section is intended to provide a comprehensive description of these concepts, nor are the examples an exhaustive account of these phenomena. Instead, this framework is intended to be a provocation and a 2020-21 highlight reel of “smart city” technology cautionary tales, with all of the cited examples taking place in the last year. While this report is primarily intended for a U.S. audience, these problems are global, and these cautionary tales come from around the world to highlight that these harms are not hypothetical but happening today. My hope for readers is to consider these possibilities, their likely ends and to build on these ideas. Totalitarianism is a form of government that controls all aspects of its people’s public and private lives and is often described by scholars as the opposite of democracy. In , Hannah Arendt articulates how the public realm is requisite to pluralism and democracy itself:In , she goes on to describe how the destruction of the public realm fuels totalitarian rule stating, “totalitarian government, like all tyrannies, certainly could not exist without destroying the public realm of life, that is, without destroying, by isolating men, their political capacities.” “Smart city” technology disrupts and destroys the public realm in our cities in three ways:A first-order question is whether the local community wants the “smart city” technology? To know the answer to this question, government representatives must facilitate ongoing engagement with community members to learn what they desire in their community and if participatory data collection is a part of their collective goals. For governments to deploy technology that can identify individuals and their activity without their will–such as affirmative consent or informed vote, or protections like warrant authorizations–should be viewed as a totalitarian act. The government should not be tracking you without a democratic discussion and decision about that.2020-21 “Smart City” Examples:The next question is, does “smart city” technology help democratic input and governance by the people? “Smart city” technology, data, and uses currently skew top-down, with government officials and their vendors deciding what is collected and how it is used, rather than a more bottom-up approach, where people willingly participate in defining collective goals and the design of any related data collection. Further, the rhetoric of data-driven government, “Moneyball for government,” behavioral nudges, and evidence-based policy popular in U.S. government management circles rest on the premise that if the government collects enough data, they can derive operational efficiencies and save money managing government programs and services. With enough data, you can “manage” the people themselves. Technocratic analysis by governments is not new, but the vast expansion of surveilling individuals, rather than engaging in dialogue with them directly through participatory means is unprecedented. This move away from participatory dialogue needs to be corrected if we want democracy to serve community needs.2020-21 “Smart City” Examples:Lastly, we must ask, does this “smart city” data collection support totalitarian qualities, such as mono-ideology, chilling of dissent, or a police state? An essential function of democracy is for the people to be able to debate different ideologies. In an increasingly watched world, the data collectors and managers set the narrative by what they choose to collect and what they do not. This narrative-setting quality can result in mono-culture or promote mono-ideologies. Because of this threat, the community should critically examine who collects the data and how they use it in story-telling. In the U.S., the First Amendment protects the five freedoms of speech, religion, press, assembly, and the right to petition (protest) the government. The surveillance imposed by “smart city” technology could have a chilling effect on community members feeling comfortable participating in these protected activities for fear of harassment or retaliation by law enforcement. Beyond the chilling of dissent, how does “smart city” technology fuel a police state? “Smart city” data supposedly collected for planning or efficiency purposes can be repurposed for enforcement purposes and currently without the requirement of review like a warrant. At what point does “safe city” rhetoric create an environment where you can effectively be tracked by law enforcement at all times?2020-21 “Smart City” Examples: Panopticonism, first proposed by English philosopher Jeremy Bentham in the 18th century and then criticized by French philosopher Michel Foucault in the 21st century, describes a system of control where prisoners don’t know when they are being watched, and thus act as though they are being watched at all times. As “smart city” technology increases the ability to watch people, it enables panopticonism in three ways:As discussed, there are many ways “smart city” technology can create a genuine panopticon by being able to track you (via your identity) and your activity (via your movement and other data) at all times. Governments are currently expanding their cameras and identifying capabilities on the ground and in the air and obtaining mobility data without a warrant. This collection of identifying data is often done in the name of public safety. Still, there have not been many high-profile cases of this new collection bringing to bear the security promised. For example, U.S. Customs and Border Protection scanned more than 23 million people in public places with facial recognition technology in 2020 and caught zero imposters. Further, while these tools have not yet fulfilled their promise, much of the rhetoric about implementing “smart city” technology, such as creating “digital twins” of community members movement around the city for government analysis or “real-time crime centers,” implies that a panopticon is the goal of these technologies. These goals should be categorically rejected. This invasion of privacy effectively creates a loss of liberty, social detriment, and chills First Amendment rights.2020-21 “Smart City” Examples:As “smart city” technology collects more and more identifying data, the ability to learn sensitive information by searching across these databases and joining these datasets also increases. The U.S. Constitution’s Fourth Amendment provides a “right of the people to be secure in their persons, houses, papers, and effects, against unreasonable searches and seizures” by the government. New cases asserting Fourth Amendment search violations in the digital age have slowly been changing Fourth Amendment doctrine. In , the Supreme Court ruled that obtaining historical cell-site location information (CSLI) containing the physical locations of cell phones without a search warrant violated the Fourth Amendment. Before , the government could obtain cell phone location records without a warrant. Since then, a series of Fourth Amendment cases have been brought to challenge the bounds of that ruling. Without strict limits via the Fourth Amendment or other privacy regulations, cities are experimenting with new technologies to track individuals such as police robots.2020-21 “Smart City” Examples:As “smart city” technology collects more identifying data, the risk of individual and collective exploits of that data increases. A data breach could lead to identity theft, causing threats to one’s safety and well-being or economic loss. Further, breachers can make collective utility grids vulnerable or governments vulnerable to hackers who may hold data hostage for ransom (also known as ransomware attacks), forcing governments to pay large sums of money to protect individual and national security.2020-21 “Smart City” Examples: Discrimination is when a person is unable to enjoy their human rights on an equal basis with others because of an unjustified distinction made in policy, law, or treatment based on their race, ethnicity, nationality, class, caste, religion, belief, sex, gender, language, sexual orientation, gender identity, sex characteristics, age, health or another status. In Simone Browne, traces back the over surveillance and “hypervisibility” of Black people to the “lantern laws” of the 17th and 18th century which required slaves to carry a lantern or candle if they were walking at night without a white owner to make them more visible and trackable. “Smart city” technology continues this trend of discriminatory surveillancein three ways:As “smart city” technology continues to collect identifying data, including data related to gender, race, and religious affiliation, stewards of this data can target ethnic and religious minorities and other marginalized and oppressed communities for monitoring, analyzing, enforcement, imprisonment, torture, and in worst-case scenarios, genocide.2020-21 “Smart City” Examples:In addition to aiding the targeting of particular groups of people, “smart city” technology can exacerbate current discriminatory practices by reinforcing the spatial inequities rife in urban planning, racial inequity in police technology, and extending those inequities to digital worlds, which is often referred to as “digital redlining.” These technologies can replicate inequity, exacerbate inequitable harms, mask inequity, transfer inequity, and compromise inequity oversight. The result of facial recognition technology (which reads dark-skin and women’s faces less accurately) being placed in predominantly Black and Latino neighborhoods results in compounded bias. Potential harms that flow from disproportionate use or disparate impact include loss of opportunities, economic loss, and social determinants.2020-21 “Smart City” Examples:By contrast, there are instances where access to digital goods, including internet access, have not been equally provided to historically marginalized communities.2020-21 “Smart City” Examples:Privatization is the transfer of a public commodity or service to private ownership and control. For example, many companies offering “smart city” technology effectively privatize government infrastructure or services–either through government procurement or public-private partnership agreements–and often the land the infrastructure occupies. In addition to  public services, technology companies are also  public services, such as how the heavily venture-backed and unregulated Uber and Lyft rideshare services have shifted public transit use to rideshare services. These examples also collect proprietary data, which is often exclusively controlled by the technology companies, creating another, and perhaps the most powerful layer of privatization, the privatization of knowledge. “Smart city” technology ushers toward privatization in three ways:The privatization of city services by corporations replaces democratic decision-making with corporate profit-driven decision-making. By owning “the digital layer,” companies can decide what information is collected and how it is used. When this data informs service delivery, it effectively controls how public services are delivered rather than voting, representation, or other democratic means. This phenomenon is particularly insidious for “smart city” technologies that offer “black box” AI as part of their toolset, where not only do they control the decision-making process, but it is not visible to community members. Lastly, these proprietary data systems can create lock-in and dependency on technology companies making it harder for governments to move away from this ownership and control.2020-21 “Smart City” Examples:Many of our rights and accountability mechanisms disappear when “smart city” technology projects privatize public infrastructure and services. For example, the Fourth Amendment privacy protections of the US Constitution discussed earlier often do not apply to private-sector data collection due to the third-party doctrine. In many cases, this private sector data collection (usually attached to a transaction) is much more granular than would be allowable under privacy regulations like  or by an Institutional Review Board (IRB), which protects the rights and welfare of human research subjects. Also, where these companies are also purchasing land (Sidewalk Toronto, Amazon HQ, Under Armor’s Port Covington project), these lands are converted to private ownership and are no longer protected by the First Amendment, which can prevent protest. Despite taking on quasi-government functions, technology companies are not currently required to comply with the accountability measures built into government functions such as public records access, public audits, or consequences for elected officials if services do not meet community members’ expectations.2020-21 “Smart City” Examples:As technology companies expand their “smart city” offerings, this creates risks that users will be tracked across multiple systems. As large technology companies with ample stores of personal data and their affiliates become more engaged with “smart city” projects, this becomes more worrisome. At what point does a single corporation have “vertical integration” (in terms of identifying data) of a whole neighborhood? If a single technology company captures government technology markets, it will also effectively control the design, access, and availability of many different kinds of surveillance technologies. For example, a company that achieves platform dominance in policing would not only reap economic benefits, but would also gain enormous power over functions essential to issues of democratic policing.”2020-21 “Smart City” Examples: Technological “solutionism,” coined by Evgeny Morozov in 2014, refers to the phenomenon of trying to reframe political, moral, and irresolvable problems as solvable by quantifying, tracking, or gamifying behavior with technology. In, , Ben Green builds on this concept describing the “tech goggles” perspective as:“Smart city” technology is and reinforces solutionism in three ways:“Smart city” technology is steeped in solutionism, and its rhetoric and promotional materials are often couched in the promise of what it could solve rather than what it has demonstrably solved in similar instances. “Smart city” sales pitches argue that with more data collection, processes can inherently be made more efficient and thus solved. These claims don’t take into consideration failures in models or theories of change or the many externalities that impact a city.2020-21 “Smart City” Examples:Procuring technology and data collection systems is expensive and costly to obtain, manage, secure, and upgrade. While data can help you identify problems you were unaware of or hone in on efficiencies, these efficiencies can be dwarfed by the costs to collect more data. For example, with better traffic data collection, cities would still need policy interventions like congestion pricing or infrastructure interventions like new thruways or medians to reduce congestion or accidents; the data only helps to outline the problem, which the community is likely already aware of.2020-21 “Smart City” Examples:Finally, the use of “smart city” technology encourages and in fact requires the use of more technology. For example, many cities are investing in the infrastructure needed for 5G upgrades currently and being sold technologies to run off such a network. Similarly, cities procure one piece of technology and end up being offered add-ons, plug-ins, or enterprise services that can seem like a deal, but lock in cities to specific vendors via proprietary data and software investment and create an environment where the city must stay with that vendor because transferring vendors would result in high-costs of data transformation.2020-21 “Smart City” Examples:  While the last year has provided many cautionary trends, it has also begun to outline paths forward to prevent these harms, and more importantly, ways to think differently about how technology influences and affects our broader political goals. To protect and promote democracy, I believe we must issue regulations that immediately blunt the ability to execute these cautionary trend harms but also build capacity for evaluating how technology affects society, and fortify our democratic spaces with technology in mind.To do so, we will need digital advocates to integrate into more substantive political advocacy movements that relate to the material needs of communities, legislatures to move beyond oversight and restore eroding rights and create new rights, and technology companies to rely on business models that do not create these harmful data markets and risks.Below I outline 10 calls to action to protect and promote democracy based on current intervention strategies being deployed and leading theory in this space. While the cautionary trends included global examples, these calls to action are outlined with the current U.S. legal framework and regulations in mind. In addition to current intervention examples to build from, I have included resources and communities to build with. My hope is like the cautionary trends section of this report, is that these calls to action serve as a time capsule of the current policy options in 2020-21, but that these options expand and change through these communities and further global examples. To stop harmful “smart city” technology, data, and uses, we must prohibit the current mission creep of “smart city” technologies being available for dragnet searches by police without strict limits. To do this, we must examine how our current legal frameworks can be reinforced to consider these new risks, and where those frameworks are insufficient how we can fill those gaps with policies and practices that properly address these new risks.Current interventions that should be expanded to limit law enforcement access to identifying data include Fourth Amendment litigation and scholarship, regulation, whistle-blowing, audits by third parties, and corporate transparency reports. To meaningfully preserve the privacy protections formerly available under the Fourth Amendment, advocates and legal scholars must continue to articulate how tracking technologies create the ability to conduct dragnet searches akin to cell-phones in  and, with the degree of government and private-sector entanglement currently present in society, narrow the third-party doctrine exception to meet that reality. Outcomes like the recent Leaders of Beautiful Struggle v. Baltimore Police Department holding are encouraging examples of how new data collection technology capabilities have changed what is acceptable under past tactics. Advocates should bring all cases in violation of unreasonable searches with a warrant, but be especially mindful of opportunities to litigate the unwarranted tracking of individuals via cameras, location trackers (from phones to vehicles), and sensors because of their widespread application. Beyond Fourth Amendment litigation, legislatures must begin regulating the use of the privacy invasive tools by law enforcement, such as surveillance technology oversight laws and data sharing regulations Massachusetts’ recent law that prevents transit authorities from disclosing personal information related to individuals’ transit system use for non-transit purposes. The recent introduction of the , which requires the government to get a court order to compel data brokers to disclose data, and the , which establishes a probable cause warrant requirement for federal, state, and local law enforcement agencies to use a cell-site simulator are also encouraging. Digital advocates and investigative journalists must continue to investigate how law enforcement is obtaining identifying data from companies and using surveillance technology unbeknownst to the public and legislative representatives. Projects like BuzzFeed News’ that showcase which law enforcement agencies have used ClearviewAI and The Policing Project’s evaluation of the Baltimore spy plane are critical facts needed to effectively develop policy that will prevent these harms. Similarly, technology companies should produce transparency reports of when law enforcement have requested identifying data from them as Amazon Ring has begun to do.To support interventions like these in your community, discuss the examples listed above with your local community groups and local representatives. To consider more deeply about how surveillance technology and capitalism enable wholesale criminalization, check out Action Center on Race and the Economy’s  and STOP LAPD Spying Coalition’s To stop harmful “smart city” technology, data, and uses, we must also ensure that identifying data is not used to track, sort, or otherwise endanger certain groups of people. This includes explicit tracking of individuals of certain groups, such as being done with the Uyghur population in Xinjiang for their “correction” or with women in Lucknow for their “protection” and the implicit tracking of certain types of individuals by tracking Muslim regions or self-reinforcing predictive-policing searches. To be clear, there are many scenarios where the collection of demographic information can be in service to justice, such as when it proves unequal treatment, but unconsented continual tracking in our public streets without people’s consent is not one of them given the documented risks. Further, governments should be obligated to continually evaluate whether these technologies have a discriminatory effect.Current interventions that should be expanded to end high-tech profiling include indictments and sanctions for human rights violations, regulation, equity impact assessments, responsible data practices, audits, and corporate refusal to sell to governments for these purposes. Amnesty International has articulated facial recognition technology’s human rights violations and called for it to be banned. The Paris Judicial Court’s Crimes Against Humanity and War Crimes unit has indicted senior executives at Nexa Technology for the company’s sale of surveillance software over the last decade led to authoritarian regimes in Libya and Egypt that resulted in the torture and disappearance of dissidents and other human rights abuses and the U.S. Department of Commerce sanctioned 14 Chinese technology companies over links to human rights abuses against Uyghur Muslims in Xinjiang, including DeepGlin who is backed by a top Silicon Valley investment firm. Legislatures like Washington’s have included anti-discriminatory measures in their proposed protecting those who fail to opt-in and Seattle’s surveillance technology law requires Equity Impact Assessments be conducted for all surveillance technologies as part of their oversight requirements. The calls for the EU to ban the use of AI in facial recognition technology that detects gender or sexuality or credit scores have also been encouraging. Public scrutiny and campaigns by civil rights organizations have successfully influenced companies like Amazon, IBM, and Microsoft to put a moratorium on facial recognition technology to governments, Huawei to backtrack a patent application they filed for a facial recognition system meant to identify Uyghur people and technology companies like Alibaba to disavow the use of their technology for targeting of ethnic groups.To support interventions like these in your community, discuss the examples listed above with your local community groups and local representatives. To consider more deeply how these tools can be expanded, check out the , led by a coalition of civil and digital rights advocacy groups, Laura Moy’s  article that features equity impact assessment application strategies, Data 4 Black Lives’ call for  and the Urban Institute’s  and To stop harmful “smart city” technology, data, and uses, we must specifically protect people from being monitored and targeted, which means we must collectively think more carefully about how to minimize the creation of identifying data that can be abused. In addition to strictly limiting access to identifying data by law enforcement and ending high-tech profiling, we must also reduce the attack surface of potential abuses by law enforcement, corporations, and nefarious actors, by minimizing the collection of identifying data everywhere, full stop. To address this, we must explore policies that consider minimizing the creation, storage, and standardization of identifying data and regulate its use. Given the entanglement of private-sector and government surveillance, including governments growing dependence on data brokers, to be successful, these policies must consider both the public and private sectors’ roles in creating, managing, and using identifying data.Current interventions that should be expanded to minimize the collection and use of identifying data everywhere include data privacy regulations, audits of misuses, demonstrations of security and other risks, and the use of methods that collect less harmful identifying data. In terms of data regulation, legislatures have begun regulating identifying technology, identifying data, and specific uses of the data, including:Some highlights of these regulations that should be expanded include the requirement of third-party review in their surveillance technology regulations, and governments expanding their commitment to evaluate data rights issues with full-time staff and governance bodies dedicated to these issues such as Privacy Officials, Commissions, and Advisory Bodies.Absent new regulation, there has been consumer protection litigation related to the use of facial recognition technology, such as when the Federal Trade Commission filed a complaint that Everalbum had deceived consumers about the use of facial recognition technology and their retention of images of users who had deactivated their accounts, protective privacy policies and standard contractual clauses related to data rights, such as those offered by Johns Hopkins’ Center for Government Excellence to ensure data is retained by governments as open data and those adopted by the EU to govern exchanges and international transfers of personal data. There have also been calls for minimizing data-sharing agreements across government agencies, such as when the Electronic Privacy Information Center (EPIC) urged a comprehensive review of DHS’s Information Sharing Access Agreements.In addition to refusing to sell identifying tools to police, technology companies are developing new methods and strategies to reduce the amount of identifying data that is created. Technology companies are creating tools to minimize the identifying capacity of images such as image altering tools (like Fawkes, which image cloaks by subtly changing pixels, or Everest Pipkin, which strips images of identifying metadata, or Anonymizer, CycleGAN, and Deep Privacy, which use GAN escape detection to create fake derivative images that look similar to the naked eye), camera applications (like Anonymous Camera, which blurs and pixelates images and strips images of identifying metadata), and video anonymization software (like Brighter.ai or FaceBlur). Technology companies such as Apple are using the power of their App store approval to provide a new AppTrackingTransparency feature that allows users to opt-out of tracking by applications on their phone. While technology tools can aid in data minimization, technology companies are still beholden to profit incentives, as exhibited by Apple’s decision to abandon encryption technology, digital keys, and data maintenance to Chinese state employees.Advocates have also minimized data collection through obfuscation techniques such as computer vision dazzle makeup techniques that confound facial recognition technology, disabled phone tracking, concealed messages through stenography, disappearing messages, and encrypted messaging applications. These obfuscation techniques should be used as demonstrative campaigns–not long-term policy solutions–and used with caution as they may be penetrable and cause more suspicion and surveillance by police.To support interventions like this in your community, you can join campaigns to regulate identifying technologies and data, like facial recognition technology, organized by your local ACLU chapter or EFF’s Electronic Frontier Alliance or join global campaigns hosted by Amnesty International and Access Now. To track recent data privacy regulations can check out JD Supra’s r, National Conference of State Legislature’s round up. To protect your identifying data, you can check out resources like EFF’sFinally, To stop harmful “smart city” technology, data, and uses, we must ensure there are consequences for using identifying data beyond our politically negotiated standards and that those consequences provide proportional redress for those harmed. It is not enough to pass data regulations if they are not enforced. Further, and especially important at this time, with many data regulation gaps, we must ensure that those who are harmed have swift and proportional channels of justice. The status quo of providing limited causes of action for those misidentified, no redress for those overly surveilled, and nominal damages to victims of data breaches is not enough.Current interventions for data regulation enforcement that should be expanded include the human rights injunctions and sanctions mentioned above, fines for violations (like the GDPR), and the cancelation of contracts with violating companies, public sector employee discipline and criminal penalties, private rights of action for those harmed, and suppression of evidence inappropriately obtained that are available under some regulations. These enforcement provisions are bolstered by whistleblower protections to encourage folks to come forward with knowledge of such misuses. Legislatures should be mindful of scoping what constitutes a misuse of identifying data and penalize it appropriately (for example, recently found that misuse of databases that one was otherwise authorized to use would not violate the , which comes with steep criminal penalties). In addition to penalties for misuse, individuals must have recourse when their identifying data has been misused against them. Private rights of action under surveillance technology laws are available in Berkeley, Cambridge, Davis, Grand Rapids, Lawrence, Oakland, San Francisco, Santa Clara, Seattle, and Somerville, and suppression remedies are available in Lawrence and Somerville. And the , which has provoked a series of lawsuits, including a recent $650 million Facebook settlement, is an encouraging example of redress.To support interventions like these in your community, discuss the examples listed above with your local community groups and local representatives. To think more deeply about how regulation can provide those harmed with justice, check out analyses of local privacy laws like Berkeley Law’s Samuelson Law, Technology, and Policy Clinic’s  that analyze available remedies and their implementation track record. To build our collective capacity to evaluate how technology impacts democracy, it is imperative we understand what our governments are doing in the first place. It is crucial to democracy to hold our government representatives accountable. This means to not only be aware of what “smart city” technologies are augmenting our neighborhoods but to firmly understand what data they are collecting and what the implications of that collection are.Current interventions for mandating transparency and legibility for public technology and data that should be expanded include government transparency regulations and practices, campaigns to watch the watchers, third-party audits, and corporate transparency reports. Legislatures have called for transparency-related “smart city” technology with surveillance technology laws and practices such as providing discoverable documentation the procurement via the Open Contracting Partnership and by documenting the call for such tools, such as Boston’s New Urban Mechanics’ Beta Blocks program, which posted a broad “Smart City” Request for Information (RFI) in 2017 and publicly posted the 100+ responses online. Beyond broadcasting proposal responses and procurement activity, governments should provide context and facilitate feedback loops related to novel technologies as Amsterdam, and Helsinki have done for AI; and Seattle has done for “surveillance technologies.”Advocates have issued campaigns to inventory surveillance technology, such as Amnesty International’s Decode Surveillance, which crowdsources the location of cameras in New York City, or EFF’s Atlas of Surveillance, which aggregates where many types of surveillance technology are located throughout the US through a variety of datasets. Beyond knowing what is being used, we must collectively understand the consequences of its use through third-party testing, evaluation, and hypotheticals. Examples of testing and evaluation include highlighting the unregulated use of facial recognition technology by Buzzfeed News’ ClearviewAI audits (mentioned above) and privacy engineers demonstrating with publicly available datasets how easy it is to re-identify commuters, such as Morgan Herlocker’s project that combined Mobility Data Specification data with other public datasets to identify sensitive scooter trips, including a midday trip from a high school in a conservative area of a city to a Planned Parenthood clinic.Lastly, technology companies like Amazon’s Ring, have begun to disclose when governments request their data publicly for transparency purposes and third-party commissions like Biometrics and Forensic Ethics have called for a publicly accessible record on the collaborative uses of live facial recognition (LFR) to reduce the secrecy around public-private partnerships. Technology companies should go further in their transparency reports and include information about all data collection, storage, subcontractor data linkage, secondary uses, and provide city officials with safety options and options community residents can add into individually if they desire more insights.To support interventions like these in your community, discuss the examples listed above with your local community groups and local representatives. To become an active auditor of the technology in your community, check out my  blog post, which also includes “smart city” contract documentation from 15 jurisdictions and a public records request template to get you started. You can also join or follow advocacy groups and investigative journalists such as Access Now, ACLU, Action Center on Race and the Economy, Amnesty International, Center for Democracy and Technology, Data 4 Black Lives, EFF, EPIC, Hiljade.Kamera.rs, Lucy Parson Labs, Reclaimyourface.eu, STOP LAPD Spying Coalition, STOP Spying NY, and journalists such as Ars Technica, Gizmodo, The Markup, MIT Tech Review, OneZero, ProPublica, The Intercept, Logic Magazine, Techdirt, The Verge, and VICE.To build our collective capacity to evaluate how technology impacts democracy, we must also all take a step back and consider how power is redistributed by technology companies beyond data rights. Evgeny Morozov penned an op-ed this May, suggesting that by focusing on privacy advocacy that we may have missed some of the more extensive ways technology companies are reconstructing power. In it, he writes:We must change our thinking beyond technology efficiency solutions and reframe social challenges around the material needs of community members. To the extent data plays a role in these goals, it should be secondary and thoroughly tested, understood, and desired by community members before deploying.Current interventions for questioning technology’s role in wicked problems that should be expanded include government-led efforts to incrementally test how technology can support broader programming, advocates calls to stop the use of certain technologies, and third-party audits of technology’s efficacy. Governments have demanded more rigor from technical support to broader solutions with practices such as Boston’s New Urban Mechanics  which includes plays such as, “Solve real problems for real people, “Don’t worship efficiency,” “Better decisions, not (just) better data,” and “Platforms make us go ¯\_(ツ)_/¯.” As mentioned, advocates like Action Center on Race & the Economy have included explicit calls to end surveillance data collection and end all funding of surveillance technology in their recommendations for  coupled with broader calls to defund the police and invest in community safety. Technology companies have provided vetted “smart city” technology case studies on platforms such as Marketplace.city that demonstrate where and how they have been used.To support interventions like these in your community, discuss the examples listed above with your local community groups and local representatives. To think more deeply about technology ideology check out Morozov’s work, as well as technology critiques by scholars and thinkers featured on the  podcast and newsletter.Finally, to build our collective capacity to evaluate how technology impacts democracy we must ensure that community members can test and vet government data collection and the narratives they reinforce. This includes challenging what data is collected, identifying what data is not collected, whom it serves, and creating missing data.Current interventions to challenge data narratives that should be expanded include governments providing bottom-up tools for communities to collect their own data and community members challenging top-down data collection through obfuscation, art, and the creation of alternative datasets. Governments have explored more ways to facilitate data collection by the community rather than dictate it, such as St. Louis’ Movement Lab, where community members mapped monuments in their community to tell a bottom-up history, or Barcelona’s committment “to solve city challenges by fostering innovation through open government, towards pluralist social transformation” as part of their . Advocates have altered the way dominant datasets are created about them by generating “data noise” by wearing adversarial fashion that features images such as fake license plates, and other techniques. Much like the limits of obfuscation techniques to minimize identifying data collection, these techniques are best used as demonstrations, with the long-term goals being formal processes to challenge and change data collection processes. Beyond demonstrating capabilities of technology, applying these tools to the powerful can be an effective advocacy tool as when Italian artist Paolo Cirio created a database with 4000 faces of French police officers to crowdsource their identification amongst protests and provoking changes in the law. Advocates have also called to record the police to create evidence and counter data narratives against those who evade accountability.To support interventions like these in your community, discuss the examples listed above with your local community groups and local representatives. To think more deeply about challenging data narratives, check out Obfuscation: A User’s Guide for Privacy and Protest by Finn Burton and Helen Nissenbaum, community participation organizations like Public Lab, research organizations such as Data & Society, and leading investigative data journalism work feature at the Investigative Reporter’s and Editor’s annual National Institute for Computer-Assisted Reporting conference. To fortify old and new democratic spaces we must create spaces for discussions about our community goals and how technology or data collection may serve those goals. We must reinforce these spaces for discussion through community organizing, legal protections, and tools that make it easier to do so. Further, to be able to meaningfully discuss community issues and applicable technology, communities need the support of trusted intermediaries, bodies of knowledge and resources to develop technology and data literacy.Current interventions to build spaces for community decision-making that should be expanded include mandated community advisory bodies, advocacy resources and organizing, and tools that facilitate discussion of social problems rather than dictate their solution. Legislatures must have an ongoing dialogue with their community members, and advisory bodies are one tool to help facilitate this. Local governments have created hyperlocal community-led advisory bodies such as New York City’s community boards or Washington D.C.’s advisory neighborhood commissions for broad policy advice. Berkeley, Oakland, San Francisco, and Seattle require independent review as part of their surveillance technology oversight. Digital advocacy organizations like the ACLU, EFF, and the Future for Privacy Forum have provided digital literacy guides related to surveillance and “smart city” issues, as well as communities for advocates and practitioners to join and discuss recent technology issues. In Toronto, community members gathered to host  an impromptu call to discuss issues central to the Sidewalk Toronto proposal. Public-private member associations, like the Minnesota Connected and Automated Vehicle Alliance, are working to collectively create a privacy and security framework to guide local “smart” transportation infrastructure and vehicle projects.To support interventions like these in your community, discuss the examples listed above with your local community groups and local representatives. To think more deeply about the process behind designing community solutions, check out the To fortify old and new democratic spaces we must also work together to consider how technology can bolster new ways to be in democratic dialogue with our fellow community members and build consensus toward our collective goals.Current interventions to explore how technology and data can serve democratic goals that should be expanded include calls for data practices and policies that consider collective rights and decision-making and the building of technology tools that facilitate consensus-building. Advocates and scholars have begun to articulate how new approaches to data governance can serve collective decision-making, such as Jonathan van Geuns and Ana Brandusescu’s which explores a taxonomy of data governance approaches, Salome Viljoen’s which considers data’s collective purposes, and Bennett Cyphers and Cory Doctorow’s which calls for open standards to minimize the corporate concentration of data control. Government’s such as Taiwan have provided tools such as Pol.is to facilitate consensus building among community members to inform policy. Technology companies, like Remix, which creates editable streets, facilitate community collaboration on imagining changes to their neighborhood.To support interventions like these in your community, discuss the examples listed above with your local community groups and local representatives. To think more deeply about how we might reconceptualize data to serve collective goals, check out  by Rinaldo Walcott,  by Maggie Walter and Tahu Kukutai, and  by Julie E. Cohen.Finally, to fortify old and new democratic spaces, we must imagine new ways of governing by the will of the people and develop new rights that serve those ends. To do this, we may need to rethink long-held frameworks (such as, but not limited to Fourth Amendment doctrine, anti-trust law, conceptions of personally identifiable information, property, and individual rights, and capitalism itself) that do not translate to the modern world. This will require robust and continuous dialogue and creative thinking about how technology relates to supporting the many and not the few.Current interventions to develop new democratic rights in the wake of technological capabilities that should be expanded include interdisciplinary evaluations, articulating new risks, challenging old paradigms, and science fiction. In , Rashida Richardson and Amba Kak create a definitional and analytical framework for understanding an ever-evolving ecosystem of technologies that consider the technical, legal, political economy, organizational, and social outcomes based on examples from around the world. In Shoshana Zuboff articulates the potential behavioral futures economy being created by an unprecedented amount of personal behavior data collection by the private sector aimed at increasing sales that she argues will not be addressed by current privacy or antitrust legal frameworks. In , the authors identify ways technology can serve to “rethink” social issues by acting as a diagnostic, a formalizer, a rebuttal, or a synecdoche. Finally, as we think about potential outcomes of current “smart city” trajectories, the science fiction work of Aldous Huxley, George Orwell, Ursula K. Le Guin, Ted Chiang, N.K. Jemisin and many others serve as ways for us to stretch our imagination of how society may rearrange itself as new technologies present themselves.To support interventions like these in your community, discuss the examples listed above with your local community groups and local representatives and let the world know what has stretched your imagination.  Larry Buchanan, Quoctrung Bui & Jugal K. Patel, , The New York Times, July 3, 2020, . Jesse Marx, , Voice of San Diego, June 29, 2020, . Sarah Holder, , Bloomberg CityLab, Aug. 6, 2020, . Voice of San Diego, , Voice of San Diego, July 22, 2020, . Kate Cox, , Ars Technica, Nov. 6, 2020, . Sarah Wray, , Cities Today, Nov. 11, 2020, . Jesse Marx, San Diego Held Back Materials Sought by Congress on Facial Recognition, Voice of San Diego, May 3, 2021, . GlobalData Thematic Research, , Verdict, Updated July 6, 2020, . Alexis Hancock, , Electronic Frontier Foundation, May 6, 2021, . Freddy Martinez & Lucy Parsons Labs, , South Side Weekly, Apr. 28, 2021, . Robert Muggah & Greg Walton,  Foreign Policy, Apr. 17, 2021, . Boston Dynamics, ,Boston Dynamics, . Frank Hansen, . YouTube, . Boris Lubarsky, , Georgetown Law Technology Review (2017), . John Czajka, et al., , ASPE (2015), . Tom Simonite, , Wired, Apr. 20, 2021, . Chris Buckley & Paul Mozur, , The New York Times, May 22, 2019, . Byron Tau, , Wall Street Journal, Dec. 9, 2020. . Troy Farah, , Motherboard Tech by VICE, Jan. 23, 2017, . OVO Energy,OVO Energy, Feb. 28, 2018, . Matthew Kassel, , Wall Street Journal, Feb. 27, 2019, . Jameson Spivack & Clare Garvie, , AI Now Institute, . Rebecca Williams,  Belfer Center for Science and International Affairs, Apr. 26, 2021, . Epiq, , JD Supra, Mar. 24, 2021, . Sarah Rippy, , IAPP, Updated June 21, 2021, . Office of the Privacy Commissioner of Canada, , Feb. 2, 2021, . An Act Authorizing and Accelerating Transportation Investment. 2020 Mass. Acts Chapter 383. Hannah Arendt, The Human Condition (1958) Hannah Arendt, The Origins Of Totalitarianism (1951) Privacy International, , Privacy International, . Christine Ferretti, , Detroit News, Nov. 16, 2020, . Samuel Axon, , Ars Technica, May 7, 2021, . Sam Prickett, , WBHM 90.3 BIRMINGHAMWATCH, Dec. 30, 2020, . Pramit Chatterjee, , Mashable India, Jan. 22, 2021, . Sidney Fussell, , Wired, . Larry Buchanan, Quoctrung Bui & Jugal K. Patel, note 1. Dave Maass and Matthew Guariglia, , Electronic Frontier Foundation, July 27, 2020, . Sam Biddle, , The Intercept, Feb. 16 2021, . April Glaser, , NBC News, Jan. 10, 2021, . Kim Lyons, , The Verge, June 24, 2021, . Joseph Cox, , Motherboard Tech by VICE, Apr. 26, 2021, . George Joseph & Jeff Offenhartz, , Gothamist, Aug. 14, 2020, . Kate Cox, , Ars Technica, Aug. 19, 2020, . Mike Holden, , WPXI-TV, May 21, 2021, . Zolan Kanno-Youngs, , The New York Times, June 19, 2020, . Paul Mozur, , The New York Times, Apr.14, 2019, . Isobel Asher Hamilton, , Business Insider, Mar. 4, 2020, . Paul Mozur,  The New York Times, July 26, 2019, . Benjamin Parkin, , Financial Times, Jan. 28, 2021, ., , Haaretz, Jul. 15, 2019, . Nyan Hlaing Lin & Min Min, , Myanmar NOW, Dec. 15, 2020, . Robyn Dixon, , Washington Post, Apr. 17, 2021, . Lenart J. Kučić,  AlgorithmWatch, July 7, 2020, . Carien du Plessis, , News24, Apr. 13, 2021, . Stephen Kafeero, , Quartz, Nov. 27, 2020, )., , Gulf News, Mar. 18, 2019, . Cade Metz, , The New York Times, Dec. 5, 2020, . Homeland Security Department, , Federal Register, Nov. 19, 2020, . Rowan Moore Gerety, , MIT Technology Review, Apr. 19, 2021, . OODA Analyst, , OODA Loop, Feb. 2, 2021, . Electronic Privacy Information Center, , Electronic Privacy Information Center, Nov. 13, 2020, . Jesse Marx, note 7. Todd Feathers, , Dec. 10, 2020, Motherboard Tech by VICE, . Suhauna Hussain & Johana Bhuiyan, , Los Angeles Times, Dec. 21, 2020, . Michael Isaac Stein, , GovTech, Nov. 17, 2020, . Chad Marlow, , American Civil Liberties Union, Dec. 23, 2020, . The Surveillance Technology Oversight Project, , S.T.O.P. - The Surveillance Technology Oversight Project, Updated June 18th, 2020, . Dave Gershgorn, , OneZero, . Global MSC Security, , Security Informed, 21 Jan. 21, 2021, . Avdhesh Mallick, , Free Press Journal, Jan. 20. 2021, . Nikhil Bhardwaj, , Tribuneindia News Service, Dec. 16, 2020, . Anu P. Lohumi, , Tribune India News Service, Apr. 6, 2021, . Nalla Babu, , The Times of India, Jan. 25, 2021, . Varsha Rani, , VICE, Nov. 27, 2020, . Asish Mehta, , The New Indian Express, Jan. 24, 2021, . The Policing Project, am, The Policing Project, Dec. 12, 2020, . Cade Metz, , The New York Times, Dec. 5, 2020, . Dave Maass & Matthew Guariglia, , Electronic Frontier Foundation, Nov. 19, 2020, . Open Media, , Meduza, Nov. 24, 2020, . Verónica Arroyo, , Access Now, Dec. 16, 2020, . Laura Carrer, Riccardo Coluccini, & Philip Di Salvo, , Wired.it, June 9, 2020, . Ashok Kumar, , The Hindu, Jan. 3, 2021, . Daniel O’Connor, , The Setonian, May 6, 2021, . Verónica Arroyo, note 80. Yuen Meikeng, , The Star, Jan 10, 2021, . Tracy Qu, , South China Morning Post, Dec. 6, 2020, . Press Release, , Electronic Frontier Foundation, June 8, 2020, . Will Doran & Danielle Battaglia, , The News & Observer, Apr. 21, 2021, . Ch. 2021-165, Laws of Fla. (2021). United States of America v. Hammond, 3:18-cr-00005, No. 45 (N.D.Ind. Oct. 24, 2018)) Leaders of a Beautiful Struggle v. Baltimore Police Department, No. 20-1495 (4th Cir. 2021) Tim Prudente, , Baltimore Sun, June 25, 2021,  Brief of The Policing Project as Amici Curiae In Support of Neither Party And In Support of Rehearing En Banc, Leaders of a Beautiful Struggle v. Baltimore Police Department, No. 20-1495 (4th Cir. 2021) Justin Sanchez et al v. Los Angeles Department of Transportation et al, 2:20-cv-05044, No. 1 (C.D.Cal. Jun. 8, 2020) Andrew Guthrie Ferguson, , Iowa Law Review, 106 Iowa L. Rev. 47, (2020). Matthew Guariglia, , Electronic Frontier Foundation, Jan. 7, 2021, . Skip Descant, , GovTech, Dec. 8, 2020, . Karen Trapenberg Frick, et al., , UC Berkley Center for Long-term Cybersecurity, Feb. 2021, . Dan Goodin, , Ars Technica, Mar. 3, 2021, . Russell Brandom, , The Verge, May 17, 2021, . Andrea Guzman, , Mother Jones, Jan. 6, 2021, . Andy Greenberg, a, Wired, Jan. 12, 2021, . Sean Hollister, , The Verge, May 13, 2021, . Samantha Schwartz, , Smart Cities Dive, Feb. 12, 2021, . Chris Teale,  Smart Cities Dive, Mar. 8, 2018, . Kevin Collier, , NBC News, May 11, 2021, . Chris Teale, , Smart Cities Dive, Apr. 6, 2020, . Press Release, , U.S. DEPARTMENT OF DEFENSE, June 3, 2021, . Adam Ismail, rk, Jalopnik, Apr. 26, 2021, . Amnesty International, ?, Amnesty International (2021), . Drew Harwell & Eva Dou, , Washington Post, Dec. 8, 2020, . Jane Wakefield, , BBC News, May 26, 2021, . Yael Grauer, , The Intercept, Jan. 29, 2021, . Raymond Zhong, , The New York Times, Dec. 16, 2020, . Johana Bhuiyan, , Los Angeles Times, Feb. 9, 2021, . Drew Harwell & Eva Dou,  note 112. Joseph Cox, , Motherboard Tech by VICE, Jan. 11, 2021, . Donald Maye, , IPVM , July 5, 2021, . Raphael Tsavkko Garcia, , OneZero, Jan. 21, 2021, . Anushka Jain, , Feminism In India, Apr. 16, 2021, . Nick Cumming-Bruce, , The New York Times, Nov. 27, 2020, ., Wikipedia, Dec. 23 2020, . Laura Moy, , Illinois Law Review, 2021 U. Ill. L. Rev. 139, (2021). Grace Huckins, , Wired, Jan. 6, 2021, . Alejandra Hayon, , Pagina 12, Aug. 3, 2019, . Bruna Fantti, O DIAOct. 7, 2019,. Drew Harwell, , Washington Post, Apr.13, 2021, . Matt Bruce,  Atlanta Black Star, May 11, 2021, . Patrick Grother, Mei Ngan & Kayee Hanaoka,  NIST IR 8280 (2019), . Larry Hardesty, , MIT News | Massachusetts Institute of Technology, Feb. 11, 2018, . Natasha Dailey, , Business Insider, Jan. 6, 2021, . Sebastian Klovig Skelton, , Computer Weekly, Jan. 8, 2021, . Jamie Kalven, , The Intercept, Apr. 13, 2021, . Todd Feathers, , Motherboard Tech by VICE, July 19, 2021, . Gracie Todd, , CNS Maryland, Nov. 19 2020, . Jeremy Ney, , American Inequality, Mar. 31, 2021, . Lola Fadulu, , The New York Times, Sept. 24, 2019, . H.R.4008 - 116th Congress (2019-2020): No Biometric Barriers to Housing Act of 2019, H.R.4008, 116th Cong. (2019), . Ella Fassler, , OneZero, Apr. 7, 2021, . Sarah Brayne, Predict and Surveil: Data, Discretion, and the Future of Policing (2020). Ernesto Falcon, , Electronic Frontier Foundation, Jan. 11, 2021, . Annie Correal, , The New York Times, Dec. 6, 2019, . Civil Rights Defenders News,  Civil Rights Defenders, June 18, 2020, . Eliska Drapalova & Kai Wegrich, , 22 Public Management Review 668–686 (2020), . Riley Snyder & Michelle Rindels, , The Nevada Independent, Feb.3, 2021, . Sidewalk Toronto, , Sidewalk Toronto, Nov. 5, 2019, . Bianca Wylie, , Boston Review, May 13, 2021, . Chris Burt, Biometric Update, May 11, 2021, . Todd Feathers, , The Markup, Apr. 15, 2021, . Alfred Ng & Maddy Varner, , The Markup, Apr. 1, 2021, . H.R.2575 - 116th Congress (2019-2020): AI in Government Act of 2020, H.R.2575, 116th Cong. (2020), . S.1558 - 116th Congress (2019-2020): AI–IA, S.1558, 116th Cong. (2019), . H.R.1282 - 116th Congress (2019-2020): Data Accountability and Trust Act, H.R.1282, 116th Cong. (2019),  H.R.6675 - 116th Congress (2019-2020): Data Broker Accountability and Transparency Act of 2020, H.R.6675, 116th Cong. (2020), . H.R.2013 - 116th Congress (2019-2020): Information Transparency & Personal Data Control Act, H.R.2013, 116th Cong. (2019), . Bianca Wylie, , ReSITE (2019), . Kim Lyons, , The Verge, Jan. 31, 2021, . Ryan Mac, et al. , BuzzFeed News, Updated Apr. 9, 2021, . Caroline Haskins & Ryan Mac, , BuzzFeed News, June 30, 2021, . Joseph Cox, note 118. Justin Sherman, , Wired, Apr. 13, 2021, . Joseph Cox, , Motherboard Tech by VICE, Mar. 3, 2021, . Sam Biddle, , The Intercept, May 3, 2021, . Joseph Cox, ”, Motherboard Tech by VICE, June 10, 2021, . Joseph Cox, , Motherboard Tech by VICE, Mar. 17, 2021, . Open Mobility Foundation, Open Mobility Foundation (2021), . Drew Harwell, Washington Post, Feb. 26, 2021, . Elizabeth E. Joh & Thomas Wuil Joo, , Denver Law Review Forum, Forthcoming, Apr. 26, 2021, . Postmedia News, ‘, Toronto Sun, Dec. 4, 2020, . Ben Green, The Smart Enough City: Putting Technology in Its Place to Reclaim Our Urban Future (2019), . Madeleine Wattenbarger, , Rest of World, Jan. 19, 2021, . Kristin Musulin, , Smart Cities Dive, Jan. 15, 2021, . Jason Koebler & Joseph Cox, , Motherboard Tech by VICE, Mar. 30, 2021, . Ryan Johnston, , StateScoop, Jan. 25, 2021, . Peachtree Corners, , PRNewswire, Jan. 15, 2021, . Brandon Vigliarolo, , TechRepublic, Jan. 19, 2021, . Bobbie Johnson, , MIT Technology Review, Nov. 23, 2020, . Charlie McGee, , Rolling Stone, May 5, 2021, . Ella Fassler,  note 140. Dorian Hargrove, et al, Memo Reveals Huge Cost Overruns For San Diego’s ‘Smart Streetlights’, NBC 7 San Diego, Feb. 15, 2020, . Leaders of a Beautiful Struggle v. Baltimore Police Department, No. 20-1495 (4th Cir. 2021) An Act Authorizing and Accelerating Transportation Investment. 2020 Mass. Acts Chapter 383. S.1265 - 117th Congress (2021-2022): Fourth Amendment Is Not For Sale Act, S.1265, 117th Cong. (2021), . S.2122 - 117th Congress (2021-2022): Cell-Site Simulator Warrant Act of 2021, S.2122, 117th Cong. (2021), . Action Center on Race and the Economy and The Community Resource Hub for Safety and Accountability, , Action Center on Race and the Economy, Apr. 19, 2021, . STOP LAPD Spying Coalition and the Free Radicals, , STOP LAPD Spying, Mar. 2, 2020, . Amnesty International and partners, , Amnesty International, June 7, 2021, . Patrick Howell O’Neill, , MIT Technology Review, June 22, 2021, . Dave Gershgorn, , The Verge, July 9, 2021, . David Stauss, , JD Supra, Feb. 1, 2021, . Rebecca Williams, note 23. Karen Hao, , MIT Technology Review, Oct. 22, 2018, . Karen Hao, , MIT Technology Review, June 12, 2020, . ANI, , Business Standard India, January 15, 2021, . Benzinga, , CFO, Dec. 18, 2020, . American Civil Liberties Union, et al., , Civil Rights Privacy and Technology Table (2020), . Laura Moy,  note 124, Jamelle Watson-Daniels, Introducing #NoMoreDataWeapons, Data for Black Lives Blog, Feb. 26, 2021, . Megan Randall, Alena Stern, & Yipeng Su, , Urban Institute, Mar. 2021, . Alena Stern, Graham MacDonald, & Khuloud Odeh. Creating Equitable Technology Programs: A Guide for Cities, Urban Institute, Sept. 2020, . Justin Sherman, note 162. Jameson Spivack & Clare Garvie,  note 22. Nicole Ozer, Kate Ruane, & Matt Cagle, ., American Civil Liberties Union , June 17, 2021, . S.2052 - 117th Congress (2021-2022): Facial Recognition and Biometric Technology Moratorium Act of 2021, S.2052, 117th Cong. (2021), . Dave Maass & Hayley Tsukayama, , Electronic Frontier Foundation, Mar. 19, 2021, . Rebecca Williams, note 23. S.2122 - 117th Congress (2021-2022): Cell-Site Simulator Warrant Act of 2021, S.2122, 117th Cong. (2021), . Epiq,  note 24. Taylor Hatmaker, , TechCrunch, Sept. 9, 2020, . Zack Whittaker,  TechCrunch, July 9, 2021, . Pam Greenberg, , National Conference of State Legislatures, Jan. 17, 2021, . Aaron Nicodemus, , Compliance Week, July 8, 202, . S.1265 - 117th Congress (2021-2022): Fourth Amendment Is Not For Sale Act, S.1265, 117th Cong. (2021), . S.2134 - 117th Congress (2021-2022): Data Protection Act of 2021, S.2134, 117th Cong. (2021), . Internet of Things Cybersecurity Improvement Act of 2020, 15 U.S. Code § sections 278g–3a to 278g–3e. Office of the Privacy Commissioner of Canada, note 27. An Act Authorizing and Accelerating Transportation Investment. 2020 Mass. Acts Chapter 383. Rebecca Williams, note 23. Press Release, , Federal Trade Commission, Jan. 11, 2021, . LinkNYC, . GovEx, GovEx Labs, . Sheila A. Miller & Tracy P. Marshall, , The National Law Review, July 7, 2021, . Electronic Privacy Information Center, , Electronic Privacy Information Center, May 14, 2021, . Sejuti Das, , Analytics India Magazine June 20, 2020, . Brian X. Chen, , The New York Times, Apr. 26, 2021, . Jack Nicas, Raymond Zhong & Daisuke Wakabayashi, , The New York Times, May 17, 2021, . Pavlina Pavlova, Open Global Rights, Nov. 20, 2020, . Lily Hay Newman, , Ars Technica, Jan. 15, 2021, . American Civil Liberties Union (2021), . Electronic Frontier Foundation (2021), . Lauren Caisman, Amy de La Lama, & Melissa Ruth Whigman, , JD Supra, May 13, 2021, . Pam Greenberg, a note 212. Alexis Hancock, note 9. Privacy Affairs, GDPR Fines List: Find all GDPR fines & detailed statistics, PRIVACY Affairs (2021), . United States v. Mayweather, No. 17-13547 (11th Cir. 2021) Rebecca Williams, note 23. Nicholas Iovino, , Courthouse News Service, Aug. 20, 2020, . Ari Chivukula & Tyler Takemoto, , Berkeley Samuelson Law, Technology & Public Policy Clinic, Feb. 2021, . Open Contracting Partnership (2021), . City of Boston (2021), . City of Boston, . City of Amsterdam, . City of Helsinki, . City of Seattle, ., Amnesty International, . Electronic Frontier Foundation, Atlas of Surveillance, Electronic Frontier Foundation,  Harry Campbell, , Ride Share Guy, Dec. 17, 2019, . Ring, The Ring Blog, June 3, 2021, . Sebastian Klovig Skelton, , Computer Weekly, Jan. 29, 2021, . Rebecca Williams, , Belfer Center for Science and International Affairs, May 25, 2021, . Evgeny Morozov,  the Guardian, May 15, 2021, . Mayor’s Office of New Urban Mechanics, Boston.gov. Action Center on Race and the Economy and The Community Resource Hub for Safety and Accountability, note 186. Lyndsay Knecht, , NEXT CITY, Jan. 27, 2021, . Corey Recvlohe, , Dialogue & Discourse, Dec. 8, 2020, . Adversarial Fashion, Adversarial Fashion, . DJ Pangburn, , Motherboard Tech by VICE, Nov. 16, 2016, . Censored work showing faces of 4,000 French police officers goes on show in Berlin, . The Associated Press, CBC News Nov. 30, 2020, . Andrew Couts, , Gizmodo, Apr. 21, 2021, ., Some Thoughts (2021), . Jule Pattison-Gordon, , GovTech, Apr. 13, 2021, ., Design Justice Network, . Mozilla Insights, Jonathan van Geuns, & Ana Brandusescu, , Mozilla, Sept. 2020, . Salome Viljoen, , Yale Law Journal, Forthcoming, Nov. 11, 2020, . Bennett Cyphers & Cory Doctorow, , Electronic Frontier Foundation, Jan. 2021, . Rashida Richardson & Amba Kak, , University of Michigan Journal of Law Reform, Vol. 55, Forthcoming, June 1, 2021, . Shoshana Zuboff, The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power (2019). Rediet Abebe et al., , Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency 252–260 (2020), .