As data sets get larger and more complex, traditional data-processing methods are increasingly unable to deliver insights in a timely fashion. The world at the edge of a global pandemic is a timely example: the analysis of millions of medical records—including disease, treatment, and genomic information—could yield critical insights for doctors, researchers, and pharma companies that are exploring novel vaccine opportunities. Yet this highly complex task frequently overwhelms traditional computing.In the healthcare industry, processing tens of millions of records to derive inferences and extract insights has been nearly impossible to date. With the further proliferation of data privacy and data localization, this challenge has increased even further. Anonymizing the same data sets while minimizing information loss has been a nightmare, requiring supercomputers to execute the workload. Time-critical applications such as digital health, where query run times can influence life-or-death decisions, have been practically impossible.For such use cases, graphical processing units (GPUs) could be game changers. GPUs are ideal for situations where complexity has so far prohibited fast and precise insights. Their massive parallelization of tasks enables a thousandfold improvement, far beyond the single-digit increases of even the most powerful traditional computing. Newer enterprise GPU chipsets, such as the NVIDIA Ampere A100, can provide up to 70 times more cores for a fraction of the price of equivalent central processing units (CPUs).Technological advancements have lowered the barriers to harnessing GPUs, providing healthcare companies with a valuable opportunity to accelerate tasks and harness data as never before. The falling costs of cloud-provider offerings and the increased ability to efficiently use the underlying hardware (from less than 10 percent for the algorithms and operating systems used before to 50 percent or more) have been pivotal to breaking the deadlock and making GPUs economical. In addition, cloud providers are beginning to build their own GPU chips to support this trend and the anticipated consumer demand. Previously seen as a niche hardware component useful only for special use cases, GPUs are now getting a second look from healthcare companies.Over the past decade, the compute performance of GPUs has significantly increased, while their cost, as measured by FLOPS, has decreased (Exhibit 1). This trajectory has been fueled by growing demand from companies and private consumers alike. Falling costs illustrate a critical opportunity, from both a business and a technology perspective, to unlock new use cases. For example, enumeration problems, which require rapid processing of exponentially increasing combinations, can quickly exceed traditional compute limits.In a traditional advanced analytics use case, CPUs handle all computations. Theoretically, companies could assemble thousands of CPU cores in large horizontal or vertical clusters for extremely complex scenarios, but since the cost will also scale, such solutions aren’t economically feasible.The traditional approach to deploying GPUs sought to use them as a tool to optimize and increase the clock speed of CPUs. But now the focus is on parallelizing compute-heavy problems and capitalizing on the ability of GPUs to process large data sets quickly, which comes at a fraction of the price of traditional CPU cores.An architecture comparison of GPUs and CPUs illustrates how certain use cases are ideal for GPUs’ high-compute density and throughput (Exhibit 2). Here, GPUs extend existing compute setups and therefore accelerate certain use cases. But not all use cases currently handled by CPUs can be successfully mapped to GPUs. While some will perform more efficiently, others will do better by staying with CPUs.Novel GPU acceleration could achieve near-real-time insights for a fraction of the current execution cost by significantly increasing computational performance through massive parallelization. Solving the challenge of anonymizing data to ensure patient privacy would be a by-product. In this setting, a single GPU chipset accelerates processing by a factor of 30 to 50, depending on the actual data structure.A look at the use of GPUs in genome sequencing reinforces their potential. Comparing a genome’s nucleobase to a reference or anonymizing data when aggregating patient medical records exponentially increases the combinations that need to be processed quickly, draining traditional compute resources. GPUs enable new use cases while reducing costs and processing times by orders of magnitude (Exhibit 3). Such acceleration can be accomplished by shifting from a scalar-based compute framework to vector or tensor calculations. This approach can increase the economic impact of the single use cases we studied by up to 40 percent.Advances in hardware have made it possible to address state-of-the-art problems that previously would have been impractical or even impossible. For instance, the promise of new high-speed-processing architectures means that healthcare companies could quickly and efficiently preprocess large genome-sequence data sets while preserving data privacy.Data privacy and protection are massive topics within healthcare, and their requirements have increased over time. The specific challenge in healthcare is that patient data does not always come in a structured form but may include numerous types of unstructured data such as pictures, records, and scans.Ensuring appropriate anonymization with standard algorithms is typically a parallelizable task requiring large-scale compute power—specifically for unstructured data sets. For example, for the identification of patient data in a picture, certain parts must be blurred to make it sharable. These targeted means of anonymization allow data exchange while meeting privacy requirements. The parallelizable tasks behind the typical algorithms used in this field (for example, k-anonymity) again make GPUs an ideal candidate for the underlying infrastructure. The same holds true for ensuring privacy in such compute-heavy areas as deep learning. Numerous technology companies are already using GPUs today for this purpose.Healthcare companies need to be thoughtful in how they harness GPUs in their organization. A three-step approach can enable them to identify promising use cases and scale them in a coordinated fashion (Exhibit 4).In the medical field, the first step in applying GPUs is to identify the key fields and use cases. For example, several biotech start-ups have focused on the potential to sequence genomes quickly with GPUs as an initial use case before moving into more elaborate ones, such as fast pattern recognition of pictures. In addition, companies could seek to address the unresolved issues of data-record anonymization in genomics. By drawing on existing data pools, companies can find root causes of rare diseases, such as multiple sclerosis or mutations of COVID-19, extremely quickly, understand their development, and home in on their vulnerabilities to support treatment. This process of identifying the key use cases includes mapping opportunities against the business value and existing corporate strategy.Second, companies must determine which enabling hardware, including resources, knowledge, and vendors for GPU-accelerated ventures, is needed to pursue priority opportunities. Typically, companies will first leverage service providers, including large cloud providers, before embarking on their own hardware installations. Last, companies looking to scale these efforts must attract and educate talent—a critical enabler for efficiently harnessing the underlying technology.Healthcare companies that incorporate GPUs into their data and analytics efforts can create a competitive advantage or even turn impractical scenarios into real possibilities. Organizations should take concrete steps to discover, evaluate, and assess their individual opportunities. While not all use cases will be suitable for GPU acceleration, carefully chosen application fields can produce the anticipated improvements. is a senior research and data-security expert at the Hasso Plattner Institute,  is an alumnus of McKinsey’s Berlin office,  is a senior partner in the London office, and  is a partner in the Frankfurt office. Floating point operations per second (FLOPS) is a measure of computer performance.
 Amaro Taylor-Weiner et al., “Scaling computational genomics to millions of individuals with GPUs,” , 2019, Volume 20, Number 228, genomebiology.biomedcentral.com.
 See, for example, AWS Graviton Processor: aws.amazon.com/ec2/graviton.
 For more, see Roberto Di Pietro, Leonardo Jero, Flavio Lombardi, and Augusti Solanas, “GPU algorithms for k-anonymity in microdata,” , 2019, pp. 1–9, ieeexplore.ieee.org.